{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a53286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Air-Quality-Health-Alert-System\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e266f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9dd3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Air-Quality-Health-Alert-System'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    \"\"\"Model Trainer Configuration\"\"\"\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    data_transformation_dir: Path  # ADD THIS - needed to load scaler\n",
    "    model_name: str\n",
    "    target_column: str\n",
    "    \n",
    "    # XGBoost hyperparameters\n",
    "    n_estimators: int\n",
    "    max_depth: int\n",
    "    learning_rate: float\n",
    "    subsample: float\n",
    "    colsample_bytree: float\n",
    "    random_state: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Air_Quality_Health_Alert_System.constants import *\n",
    "from Air_Quality_Health_Alert_System.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.XGBOOST\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            data_transformation_dir=self.config.data_transformation.root_dir,  # ADD THIS LINE\n",
    "            model_name=config.model_name,\n",
    "            target_column=schema.name,\n",
    "            \n",
    "            n_estimators=params.n_estimators,\n",
    "            max_depth=params.max_depth,\n",
    "            learning_rate=params.learning_rate,\n",
    "            subsample=params.subsample,\n",
    "            colsample_bytree=params.colsample_bytree,\n",
    "            random_state=params.random_state\n",
    "        )\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from Air_Quality_Health_Alert_System import logger\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff5e51",
   "metadata": {},
   "source": [
    "Model Training Workflow. \n",
    "\n",
    "- Loaded the scaler.joblib from the data transformation stage \n",
    "- RandomizedSearchCV is used for hyperparameter tunning. \n",
    "- Used XGBoost to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46fd393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "\n",
    "    def load_scaler(self):\n",
    "        \n",
    "        scaler_path = os.path.join(self.config.data_transformation_dir, \"scaler.joblib\")\n",
    "        if os.path.exists(scaler_path):\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "            logger.info(f\"Scaler loaded from: {scaler_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"No scaler found at: {scaler_path}\")\n",
    "            self.scaler = None\n",
    "   \n",
    "\n",
    "    def prepare_features(self, train_data, test_data):\n",
    "        \n",
    "        \n",
    "        columns_to_drop = ['date', 'city', 'AQI_Category']\n",
    "        \n",
    "        \n",
    "        train_x = train_data.drop(columns=columns_to_drop + [self.config.target_column], errors='ignore')\n",
    "        test_x = test_data.drop(columns=columns_to_drop + [self.config.target_column], errors='ignore')\n",
    "        train_y = train_data[self.config.target_column]\n",
    "        test_y = test_data[self.config.target_column]\n",
    "\n",
    "        \n",
    "        cat_cols = train_x.select_dtypes(include=['object']).columns.tolist()\n",
    "        if cat_cols:\n",
    "            logger.info(f\"Encoding categorical columns: {cat_cols}\")\n",
    "            train_x = pd.get_dummies(train_x, columns=cat_cols, drop_first=True)\n",
    "            test_x = pd.get_dummies(test_x, columns=cat_cols, drop_first=True)\n",
    "            \n",
    "            \n",
    "            test_x = test_x.reindex(columns=train_x.columns, fill_value=0)\n",
    "\n",
    "        \n",
    "        self.feature_columns = train_x.columns.tolist()\n",
    "        \n",
    "        logger.info(f\"Feature preparation completed:\")\n",
    "        logger.info(f\"  Training features shape: {train_x.shape}\")\n",
    "        logger.info(f\"  Test features shape: {test_x.shape}\")\n",
    "        logger.info(f\"  Total features: {len(self.feature_columns)}\")\n",
    "\n",
    "        return train_x, test_x, train_y, test_y\n",
    "\n",
    "\n",
    "    def get_hyperparameter_grid(self):\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],  \n",
    "            'max_depth': [3, 5, 7], \n",
    "            'learning_rate': [0.05, 0.1, 0.2], \n",
    "            'subsample': [0.7, 0.8, 0.9],  \n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],  \n",
    "            'reg_alpha': [0, 0.1, 1],  \n",
    "            'reg_lambda': [0, 0.1, 1],  \n",
    "            'min_child_weight': [1, 3, 5],  \n",
    "            'gamma': [0, 0.1, 0.2]  \n",
    "        }\n",
    "        return param_grid\n",
    "\n",
    "    def evaluate_model(self, model, train_x, train_y, test_x, test_y):\n",
    "       \n",
    "        \n",
    "      \n",
    "        train_predictions = model.predict(train_x)\n",
    "        test_predictions = model.predict(test_x)\n",
    "        \n",
    "       \n",
    "        train_mse = mean_squared_error(train_y, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_mae = mean_absolute_error(train_y, train_predictions)\n",
    "        train_r2 = r2_score(train_y, train_predictions)\n",
    "        train_mape = np.mean(np.abs((train_y - train_predictions) / train_y)) * 100\n",
    "        \n",
    "        \n",
    "        test_mse = mean_squared_error(test_y, test_predictions)\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(test_y, test_predictions)\n",
    "        test_r2 = r2_score(test_y, test_predictions)\n",
    "        test_mape = np.mean(np.abs((test_y - test_predictions) / test_y)) * 100\n",
    "        \n",
    "        metrics = {\n",
    "            'train': {\n",
    "                'mse': train_mse,\n",
    "                'rmse': train_rmse,\n",
    "                'mae': train_mae,\n",
    "                'r2': train_r2,\n",
    "                'mape': train_mape\n",
    "            },\n",
    "            'test': {\n",
    "                'mse': test_mse,\n",
    "                'rmse': test_rmse,\n",
    "                'mae': test_mae,\n",
    "                'r2': test_r2,\n",
    "                'mape': test_mape\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "        logger.info(\"=== Model Evaluation Metrics ===\")\n",
    "        logger.info(\"TRAINING SET:\")\n",
    "        logger.info(f\"  MSE: {train_mse:.4f}\")\n",
    "        logger.info(f\"  RMSE: {train_rmse:.4f}\")\n",
    "        logger.info(f\"  MAE: {train_mae:.4f}\")\n",
    "        logger.info(f\"  R²: {train_r2:.4f}\")\n",
    "        logger.info(f\"  MAPE: {train_mape:.2f}%\")\n",
    "        \n",
    "        logger.info(\"TEST SET:\")\n",
    "        logger.info(f\"  MSE: {test_mse:.4f}\")\n",
    "        logger.info(f\"  RMSE: {test_rmse:.4f}\")\n",
    "        logger.info(f\"  MAE: {test_mae:.4f}\")\n",
    "        logger.info(f\"  R²: {test_r2:.4f}\")\n",
    "        logger.info(f\"  MAPE: {test_mape:.2f}%\")\n",
    "        \n",
    "      \n",
    "        r2_diff = train_r2 - test_r2\n",
    "        if r2_diff > 0.1:\n",
    "            logger.warning(f\"Potential overfitting detected! Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "        else:\n",
    "            logger.info(f\"Good generalization! Train-Test R² difference: {r2_diff:.4f}\")\n",
    "        \n",
    "        return metrics, test_predictions\n",
    "\n",
    "    def save_model_artifacts(self, model, metrics=None, test_data=None):\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        \n",
    "        model_artifacts = {\n",
    "            'model': model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'target_column': self.config.target_column,\n",
    "            'model_type': 'XGBRegressor',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        \n",
    "        model_path = os.path.join(self.config.root_dir, self.config.model_name)\n",
    "        joblib.dump(model_artifacts, model_path)\n",
    "        logger.info(f\"Model artifacts saved at: {model_path}\")\n",
    "\n",
    "        if test_data is not None:\n",
    "            test_data_path = os.path.join(self.config.root_dir, \"test_data.joblib\")\n",
    "            joblib.dump(test_data, test_data_path)\n",
    "            logger.info(f\"Test data saved at: {test_data_path}\")\n",
    "            \n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': self.feature_columns,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            importance_path = os.path.join(self.config.root_dir, \"feature_importance.csv\")\n",
    "            feature_importance.to_csv(importance_path, index=False)\n",
    "            logger.info(f\"Feature importance saved at: {importance_path}\")\n",
    "            \n",
    "            \n",
    "            logger.info(\"=== Top 10 Important Features ===\")\n",
    "            for idx, row in feature_importance.head(10).iterrows():\n",
    "                logger.info(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "    def train(self):\n",
    "       \n",
    "        logger.info(\"Starting model training pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            self.load_scaler()\n",
    "            \n",
    "            \n",
    "            logger.info(\"Loading training and test data...\")\n",
    "            train_data = pd.read_csv(self.config.train_data_path, parse_dates=['date'])\n",
    "            test_data = pd.read_csv(self.config.test_data_path, parse_dates=['date'])\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            train_x, test_x, train_y, test_y = self.prepare_features(train_data, test_data)\n",
    "\n",
    "          \n",
    "            xgb_model = XGBRegressor(\n",
    "                tree_method=\"hist\", \n",
    "                random_state=self.config.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            \n",
    "            logger.info(\"Starting hyperparameter tuning...\")\n",
    "            param_grid = self.get_hyperparameter_grid()\n",
    "            \n",
    "           \n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "            \n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=xgb_model,\n",
    "                param_distributions=param_grid,\n",
    "                n_iter=30,  \n",
    "                scoring='neg_mean_squared_error',\n",
    "                cv=tscv,\n",
    "                verbose=1,\n",
    "                random_state=self.config.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            \n",
    "            logger.info(\"Training model with hyperparameter optimization...\")\n",
    "            random_search.fit(\n",
    "                train_x, \n",
    "                train_y,\n",
    "                eval_set=[(test_x, test_y)],\n",
    "                early_stopping_rounds=10, \n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            best_model = random_search.best_estimator_\n",
    "            \n",
    "            logger.info(\"=== Best Hyperparameters ===\")\n",
    "            for param, value in random_search.best_params_.items():\n",
    "                logger.info(f\"{param}: {value}\")\n",
    "\n",
    "            \n",
    "            metrics, predictions = self.evaluate_model(best_model, train_x, train_y, test_x, test_y)\n",
    "\n",
    "            test_data_to_save = {\n",
    "            'X_test': test_x,\n",
    "            'y_test': test_y,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "\n",
    "            \n",
    "            self.save_model_artifacts(best_model, metrics, test_data_to_save)\n",
    "\n",
    "            logger.info(\"Model training completed successfully!\")\n",
    "            \n",
    "            return {\n",
    "                'model': best_model,\n",
    "                'metrics': metrics,\n",
    "                'predictions': predictions,\n",
    "                'feature_columns': self.feature_columns\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model training: {str(e)}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9ca26cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-23 14:38:10,728: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-08-23 14:38:10,734: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-23 14:38:10,739: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-08-23 14:38:10,741: INFO: common: created directory at: artifacts]\n",
      "[2025-08-23 14:38:10,745: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2025-08-23 14:38:10,747: INFO: 4280626163: Starting model training pipeline...]\n",
      "[2025-08-23 14:38:10,749: INFO: 4280626163: Scaler loaded from: artifacts/data_transformation\\scaler.joblib]\n",
      "[2025-08-23 14:38:10,750: INFO: 4280626163: Loading training and test data...]\n",
      "[2025-08-23 14:38:12,205: INFO: 4280626163: Encoding categorical columns: ['season']]\n",
      "[2025-08-23 14:38:12,324: INFO: 4280626163: Feature preparation completed:]\n",
      "[2025-08-23 14:38:12,327: INFO: 4280626163:   Training features shape: (91953, 43)]\n",
      "[2025-08-23 14:38:12,328: INFO: 4280626163:   Test features shape: (30651, 43)]\n",
      "[2025-08-23 14:38:12,329: INFO: 4280626163:   Total features: 43]\n",
      "[2025-08-23 14:38:12,330: INFO: 4280626163: Starting hyperparameter tuning...]\n",
      "[2025-08-23 14:38:12,331: INFO: 4280626163: Training model with hyperparameter optimization...]\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SAMITH\\Github\\Air-Quality-Health-Alert-System\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-23 14:41:47,007: INFO: 4280626163: === Best Hyperparameters ===]\n",
      "[2025-08-23 14:41:47,009: INFO: 4280626163: subsample: 0.9]\n",
      "[2025-08-23 14:41:47,010: INFO: 4280626163: reg_lambda: 0]\n",
      "[2025-08-23 14:41:47,012: INFO: 4280626163: reg_alpha: 0]\n",
      "[2025-08-23 14:41:47,013: INFO: 4280626163: n_estimators: 100]\n",
      "[2025-08-23 14:41:47,015: INFO: 4280626163: min_child_weight: 3]\n",
      "[2025-08-23 14:41:47,016: INFO: 4280626163: max_depth: 3]\n",
      "[2025-08-23 14:41:47,018: INFO: 4280626163: learning_rate: 0.2]\n",
      "[2025-08-23 14:41:47,019: INFO: 4280626163: gamma: 0.1]\n",
      "[2025-08-23 14:41:47,021: INFO: 4280626163: colsample_bytree: 0.9]\n",
      "[2025-08-23 14:41:47,566: INFO: 4280626163: === Model Evaluation Metrics ===]\n",
      "[2025-08-23 14:41:47,569: INFO: 4280626163: TRAINING SET:]\n",
      "[2025-08-23 14:41:47,570: INFO: 4280626163:   MSE: 0.0049]\n",
      "[2025-08-23 14:41:47,571: INFO: 4280626163:   RMSE: 0.0697]\n",
      "[2025-08-23 14:41:47,572: INFO: 4280626163:   MAE: 0.0281]\n",
      "[2025-08-23 14:41:47,574: INFO: 4280626163:   R²: 0.2840]\n",
      "[2025-08-23 14:41:47,575: INFO: 4280626163:   MAPE: inf%]\n",
      "[2025-08-23 14:41:47,576: INFO: 4280626163: TEST SET:]\n",
      "[2025-08-23 14:41:47,577: INFO: 4280626163:   MSE: 0.0053]\n",
      "[2025-08-23 14:41:47,578: INFO: 4280626163:   RMSE: 0.0731]\n",
      "[2025-08-23 14:41:47,579: INFO: 4280626163:   MAE: 0.0289]\n",
      "[2025-08-23 14:41:47,580: INFO: 4280626163:   R²: 0.2655]\n",
      "[2025-08-23 14:41:47,586: INFO: 4280626163:   MAPE: 19.91%]\n",
      "[2025-08-23 14:41:47,588: INFO: 4280626163: Good generalization! Train-Test R² difference: 0.0185]\n",
      "[2025-08-23 14:41:47,597: INFO: 4280626163: Model artifacts saved at: artifacts/model_trainer\\model.joblib]\n",
      "[2025-08-23 14:41:47,627: INFO: 4280626163: Test data saved at: artifacts/model_trainer\\test_data.joblib]\n",
      "[2025-08-23 14:41:47,643: INFO: 4280626163: Feature importance saved at: artifacts/model_trainer\\feature_importance.csv]\n",
      "[2025-08-23 14:41:47,644: INFO: 4280626163: === Top 10 Important Features ===]\n",
      "[2025-08-23 14:41:47,647: INFO: 4280626163: o3: 0.8481]\n",
      "[2025-08-23 14:41:47,653: INFO: 4280626163: o3_wind_interaction: 0.0418]\n",
      "[2025-08-23 14:41:47,655: INFO: 4280626163: pm10_wind_interaction: 0.0391]\n",
      "[2025-08-23 14:41:47,657: INFO: 4280626163: month: 0.0291]\n",
      "[2025-08-23 14:41:47,659: INFO: 4280626163: pm25: 0.0236]\n",
      "[2025-08-23 14:41:47,661: INFO: 4280626163: no2: 0.0183]\n",
      "[2025-08-23 14:41:47,662: INFO: 4280626163: longitude: 0.0000]\n",
      "[2025-08-23 14:41:47,667: INFO: 4280626163: co: 0.0000]\n",
      "[2025-08-23 14:41:47,671: INFO: 4280626163: latitude: 0.0000]\n",
      "[2025-08-23 14:41:47,674: INFO: 4280626163: pm10: 0.0000]\n",
      "[2025-08-23 14:41:47,675: INFO: 4280626163: Model training completed successfully!]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)  \n",
    "    model_trainer.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45faa29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
