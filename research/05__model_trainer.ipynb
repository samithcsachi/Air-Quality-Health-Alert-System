{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a53286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Air-Quality-Health-Alert-System\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e266f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9dd3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Air-Quality-Health-Alert-System'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    \"\"\"Model Trainer Configuration\"\"\"\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    data_transformation_dir: Path  # ADD THIS - needed to load scaler\n",
    "    model_name: str\n",
    "    target_column: str\n",
    "    \n",
    "    # XGBoost hyperparameters\n",
    "    n_estimators: int\n",
    "    max_depth: int\n",
    "    learning_rate: float\n",
    "    subsample: float\n",
    "    colsample_bytree: float\n",
    "    random_state: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Air_Quality_Health_Alert_System.constants import *\n",
    "from Air_Quality_Health_Alert_System.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.XGBOOST\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            data_transformation_dir=self.config.data_transformation.root_dir,  # ADD THIS LINE\n",
    "            model_name=config.model_name,\n",
    "            target_column=schema.name,\n",
    "            \n",
    "            n_estimators=params.n_estimators,\n",
    "            max_depth=params.max_depth,\n",
    "            learning_rate=params.learning_rate,\n",
    "            subsample=params.subsample,\n",
    "            colsample_bytree=params.colsample_bytree,\n",
    "            random_state=params.random_state\n",
    "        )\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from Air_Quality_Health_Alert_System import logger\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "\n",
    "    def load_scaler(self):\n",
    "        \n",
    "        scaler_path = os.path.join(self.config.data_transformation_dir, \"scaler.joblib\")\n",
    "        if os.path.exists(scaler_path):\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "            logger.info(f\"Scaler loaded from: {scaler_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"No scaler found at: {scaler_path}\")\n",
    "            self.scaler = None\n",
    "\n",
    "    def add_lag_features(self, df, target_col, lags=[1, 2, 3], rolling_windows=[3, 7]):\n",
    "        \n",
    "        df = df.copy()\n",
    "        df = df.sort_values('date').reset_index(drop=True)  # Ensure proper time ordering\n",
    "        initial_rows = len(df)\n",
    "        \n",
    "       \n",
    "        for lag in lags:\n",
    "            df[f\"{target_col}_lag{lag}\"] = df[target_col].shift(lag)\n",
    "        \n",
    "      \n",
    "        for window in rolling_windows:\n",
    "            df[f\"{target_col}_rolling{window}\"] = df[target_col].shift(1).rolling(window=window).mean()\n",
    "            df[f\"{target_col}_rolling{window}_std\"] = df[target_col].shift(1).rolling(window=window).std()\n",
    "        \n",
    "        \n",
    "        df = df.dropna()\n",
    "        final_rows = len(df)\n",
    "        \n",
    "        logger.info(f\"Added lag features. Rows: {initial_rows} -> {final_rows} (removed {initial_rows - final_rows} rows)\")\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def prepare_features(self, full_data):\n",
    "        \n",
    "        \n",
    "        \n",
    "        full_data = full_data.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        full_data = self.add_lag_features(full_data, self.config.target_column)\n",
    "        \n",
    "        \n",
    "        split_ratio = 0.75  \n",
    "        split_index = int(len(full_data) * split_ratio)\n",
    "        \n",
    "        train_data = full_data.iloc[:split_index].copy()\n",
    "        test_data = full_data.iloc[split_index:].copy()\n",
    "        \n",
    "        logger.info(f\"Time-based split:\")\n",
    "        logger.info(f\"  Train period: {train_data['date'].min()} to {train_data['date'].max()}\")\n",
    "        logger.info(f\"  Test period: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
    "        \n",
    "       \n",
    "        columns_to_drop = ['date', 'city', 'AQI_Category']\n",
    "        \n",
    "        \n",
    "        train_x = train_data.drop(columns=columns_to_drop + [self.config.target_column], errors='ignore')\n",
    "        test_x = test_data.drop(columns=columns_to_drop + [self.config.target_column], errors='ignore')\n",
    "        train_y = train_data[self.config.target_column]\n",
    "        test_y = test_data[self.config.target_column]\n",
    "\n",
    "        \n",
    "        cat_cols = train_x.select_dtypes(include=['object']).columns.tolist()\n",
    "        if cat_cols:\n",
    "            logger.info(f\"Encoding categorical columns: {cat_cols}\")\n",
    "            train_x = pd.get_dummies(train_x, columns=cat_cols, drop_first=True)\n",
    "            test_x = pd.get_dummies(test_x, columns=cat_cols, drop_first=True)\n",
    "            \n",
    "            \n",
    "            test_x = test_x.reindex(columns=train_x.columns, fill_value=0)\n",
    "\n",
    "        \n",
    "        self.feature_columns = train_x.columns.tolist()\n",
    "        \n",
    "        logger.info(f\"Feature preparation completed:\")\n",
    "        logger.info(f\"  Training features shape: {train_x.shape}\")\n",
    "        logger.info(f\"  Test features shape: {test_x.shape}\")\n",
    "        logger.info(f\"  Total features: {len(self.feature_columns)}\")\n",
    "\n",
    "        return train_x, test_x, train_y, test_y\n",
    "\n",
    "    def get_hyperparameter_grid(self):\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],  \n",
    "            'max_depth': [3, 5, 7], \n",
    "            'learning_rate': [0.05, 0.1, 0.2], \n",
    "            'subsample': [0.7, 0.8, 0.9],  \n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],  \n",
    "            'reg_alpha': [0, 0.1, 1],  \n",
    "            'reg_lambda': [0, 0.1, 1],  \n",
    "            'min_child_weight': [1, 3, 5],  \n",
    "            'gamma': [0, 0.1, 0.2]  \n",
    "        }\n",
    "        return param_grid\n",
    "\n",
    "    def evaluate_model(self, model, train_x, train_y, test_x, test_y):\n",
    "       \n",
    "        \n",
    "      \n",
    "        train_predictions = model.predict(train_x)\n",
    "        test_predictions = model.predict(test_x)\n",
    "        \n",
    "       \n",
    "        train_mse = mean_squared_error(train_y, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_mae = mean_absolute_error(train_y, train_predictions)\n",
    "        train_r2 = r2_score(train_y, train_predictions)\n",
    "        train_mape = np.mean(np.abs((train_y - train_predictions) / train_y)) * 100\n",
    "        \n",
    "        \n",
    "        test_mse = mean_squared_error(test_y, test_predictions)\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(test_y, test_predictions)\n",
    "        test_r2 = r2_score(test_y, test_predictions)\n",
    "        test_mape = np.mean(np.abs((test_y - test_predictions) / test_y)) * 100\n",
    "        \n",
    "        metrics = {\n",
    "            'train': {\n",
    "                'mse': train_mse,\n",
    "                'rmse': train_rmse,\n",
    "                'mae': train_mae,\n",
    "                'r2': train_r2,\n",
    "                'mape': train_mape\n",
    "            },\n",
    "            'test': {\n",
    "                'mse': test_mse,\n",
    "                'rmse': test_rmse,\n",
    "                'mae': test_mae,\n",
    "                'r2': test_r2,\n",
    "                'mape': test_mape\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "        logger.info(\"=== Model Evaluation Metrics ===\")\n",
    "        logger.info(\"TRAINING SET:\")\n",
    "        logger.info(f\"  MSE: {train_mse:.4f}\")\n",
    "        logger.info(f\"  RMSE: {train_rmse:.4f}\")\n",
    "        logger.info(f\"  MAE: {train_mae:.4f}\")\n",
    "        logger.info(f\"  R²: {train_r2:.4f}\")\n",
    "        logger.info(f\"  MAPE: {train_mape:.2f}%\")\n",
    "        \n",
    "        logger.info(\"TEST SET:\")\n",
    "        logger.info(f\"  MSE: {test_mse:.4f}\")\n",
    "        logger.info(f\"  RMSE: {test_rmse:.4f}\")\n",
    "        logger.info(f\"  MAE: {test_mae:.4f}\")\n",
    "        logger.info(f\"  R²: {test_r2:.4f}\")\n",
    "        logger.info(f\"  MAPE: {test_mape:.2f}%\")\n",
    "        \n",
    "      \n",
    "        r2_diff = train_r2 - test_r2\n",
    "        if r2_diff > 0.1:\n",
    "            logger.warning(f\"Potential overfitting detected! Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "        else:\n",
    "            logger.info(f\"Good generalization! Train-Test R² difference: {r2_diff:.4f}\")\n",
    "        \n",
    "        return metrics, test_predictions\n",
    "\n",
    "    def save_model_artifacts(self, model, metrics=None, test_data=None):\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        \n",
    "        model_artifacts = {\n",
    "            'model': model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'target_column': self.config.target_column,\n",
    "            'model_type': 'XGBRegressor',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        \n",
    "        model_path = os.path.join(self.config.root_dir, self.config.model_name)\n",
    "        joblib.dump(model_artifacts, model_path)\n",
    "        logger.info(f\"Model artifacts saved at: {model_path}\")\n",
    "\n",
    "        if test_data is not None:\n",
    "            test_data_path = os.path.join(self.config.root_dir, \"test_data.joblib\")\n",
    "            joblib.dump(test_data, test_data_path)\n",
    "            logger.info(f\"Test data saved at: {test_data_path}\")\n",
    "            \n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': self.feature_columns,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            importance_path = os.path.join(self.config.root_dir, \"feature_importance.csv\")\n",
    "            feature_importance.to_csv(importance_path, index=False)\n",
    "            logger.info(f\"Feature importance saved at: {importance_path}\")\n",
    "            \n",
    "            \n",
    "            logger.info(\"=== Top 10 Important Features ===\")\n",
    "            for idx, row in feature_importance.head(10).iterrows():\n",
    "                logger.info(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "    def train(self):\n",
    "       \n",
    "        logger.info(\"Starting model training pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            self.load_scaler()\n",
    "            \n",
    "            \n",
    "            logger.info(\"Loading training and test data...\")\n",
    "            train_data = pd.read_csv(self.config.train_data_path, parse_dates=['date'])\n",
    "            test_data = pd.read_csv(self.config.test_data_path, parse_dates=['date'])\n",
    "            \n",
    "            \n",
    "            full_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "            logger.info(f\"Combined data shape: {full_data.shape}\")\n",
    "\n",
    "            \n",
    "            train_x, test_x, train_y, test_y = self.prepare_features(full_data)\n",
    "\n",
    "          \n",
    "            xgb_model = XGBRegressor(\n",
    "                tree_method=\"hist\", \n",
    "                random_state=self.config.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            \n",
    "            logger.info(\"Starting hyperparameter tuning...\")\n",
    "            param_grid = self.get_hyperparameter_grid()\n",
    "            \n",
    "           \n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "            \n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=xgb_model,\n",
    "                param_distributions=param_grid,\n",
    "                n_iter=30,  \n",
    "                scoring='neg_mean_squared_error',\n",
    "                cv=tscv,\n",
    "                verbose=1,\n",
    "                random_state=self.config.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            \n",
    "            logger.info(\"Training model with hyperparameter optimization...\")\n",
    "            random_search.fit(\n",
    "                train_x, \n",
    "                train_y,\n",
    "                eval_set=[(test_x, test_y)],\n",
    "                early_stopping_rounds=10, \n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            best_model = random_search.best_estimator_\n",
    "            \n",
    "            logger.info(\"=== Best Hyperparameters ===\")\n",
    "            for param, value in random_search.best_params_.items():\n",
    "                logger.info(f\"{param}: {value}\")\n",
    "\n",
    "            \n",
    "            metrics, predictions = self.evaluate_model(best_model, train_x, train_y, test_x, test_y)\n",
    "\n",
    "            test_data_to_save = {\n",
    "            'X_test': test_x,\n",
    "            'y_test': test_y,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "\n",
    "            \n",
    "            self.save_model_artifacts(best_model, metrics, test_data_to_save)\n",
    "\n",
    "            logger.info(\"Model training completed successfully!\")\n",
    "            \n",
    "            return {\n",
    "                'model': best_model,\n",
    "                'metrics': metrics,\n",
    "                'predictions': predictions,\n",
    "                'feature_columns': self.feature_columns\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model training: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9ca26cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-21 09:15:17,108: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-08-21 09:15:17,117: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-21 09:15:17,128: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-08-21 09:15:17,131: INFO: common: created directory at: artifacts]\n",
      "[2025-08-21 09:15:17,134: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2025-08-21 09:15:17,138: INFO: 3448447341: Starting model training pipeline...]\n",
      "[2025-08-21 09:15:17,144: INFO: 3448447341: Scaler loaded from: artifacts/data_transformation\\scaler.joblib]\n",
      "[2025-08-21 09:15:17,147: INFO: 3448447341: Loading training and test data...]\n",
      "[2025-08-21 09:15:19,412: INFO: 3448447341: Combined data shape: (93126, 45)]\n",
      "[2025-08-21 09:15:19,815: INFO: 3448447341: Added lag features. Rows: 93126 -> 93119 (removed 7 rows)]\n",
      "[2025-08-21 09:15:19,909: INFO: 3448447341: Time-based split:]\n",
      "[2025-08-21 09:15:19,915: INFO: 3448447341:   Train period: 2019-01-01 00:00:00 to 2022-10-01 00:00:00]\n",
      "[2025-08-21 09:15:19,920: INFO: 3448447341:   Test period: 2022-10-01 00:00:00 to 2023-12-31 00:00:00]\n",
      "[2025-08-21 09:15:19,960: INFO: 3448447341: Encoding categorical columns: ['season']]\n",
      "[2025-08-21 09:15:20,061: INFO: 3448447341: Feature preparation completed:]\n",
      "[2025-08-21 09:15:20,064: INFO: 3448447341:   Training features shape: (69839, 50)]\n",
      "[2025-08-21 09:15:20,066: INFO: 3448447341:   Test features shape: (23280, 50)]\n",
      "[2025-08-21 09:15:20,071: INFO: 3448447341:   Total features: 50]\n",
      "[2025-08-21 09:15:20,085: INFO: 3448447341: Starting hyperparameter tuning...]\n",
      "[2025-08-21 09:15:20,089: INFO: 3448447341: Training model with hyperparameter optimization...]\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SAMITH\\Github\\Air-Quality-Health-Alert-System\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-21 09:19:31,391: INFO: 3448447341: === Best Hyperparameters ===]\n",
      "[2025-08-21 09:19:31,393: INFO: 3448447341: subsample: 0.7]\n",
      "[2025-08-21 09:19:31,394: INFO: 3448447341: reg_lambda: 0.1]\n",
      "[2025-08-21 09:19:31,396: INFO: 3448447341: reg_alpha: 0.1]\n",
      "[2025-08-21 09:19:31,398: INFO: 3448447341: n_estimators: 300]\n",
      "[2025-08-21 09:19:31,400: INFO: 3448447341: min_child_weight: 5]\n",
      "[2025-08-21 09:19:31,402: INFO: 3448447341: max_depth: 7]\n",
      "[2025-08-21 09:19:31,403: INFO: 3448447341: learning_rate: 0.05]\n",
      "[2025-08-21 09:19:31,409: INFO: 3448447341: gamma: 0.1]\n",
      "[2025-08-21 09:19:31,414: INFO: 3448447341: colsample_bytree: 0.9]\n",
      "[2025-08-21 09:19:32,364: INFO: 3448447341: === Model Evaluation Metrics ===]\n",
      "[2025-08-21 09:19:32,366: INFO: 3448447341: TRAINING SET:]\n",
      "[2025-08-21 09:19:32,369: INFO: 3448447341:   MSE: 0.0059]\n",
      "[2025-08-21 09:19:32,372: INFO: 3448447341:   RMSE: 0.0771]\n",
      "[2025-08-21 09:19:32,375: INFO: 3448447341:   MAE: 0.0305]\n",
      "[2025-08-21 09:19:32,377: INFO: 3448447341:   R²: 0.2660]\n",
      "[2025-08-21 09:19:32,379: INFO: 3448447341:   MAPE: inf%]\n",
      "[2025-08-21 09:19:32,381: INFO: 3448447341: TEST SET:]\n",
      "[2025-08-21 09:19:32,383: INFO: 3448447341:   MSE: 0.0063]\n",
      "[2025-08-21 09:19:32,385: INFO: 3448447341:   RMSE: 0.0792]\n",
      "[2025-08-21 09:19:32,390: INFO: 3448447341:   MAE: 0.0309]\n",
      "[2025-08-21 09:19:32,394: INFO: 3448447341:   R²: 0.2471]\n",
      "[2025-08-21 09:19:32,399: INFO: 3448447341:   MAPE: 24.21%]\n",
      "[2025-08-21 09:19:32,401: INFO: 3448447341: Good generalization! Train-Test R² difference: 0.0189]\n",
      "[2025-08-21 09:19:32,438: INFO: 3448447341: Model artifacts saved at: artifacts/model_trainer\\model.joblib]\n",
      "[2025-08-21 09:19:32,454: INFO: 3448447341: Feature importance saved at: artifacts/model_trainer\\feature_importance.csv]\n",
      "[2025-08-21 09:19:32,462: INFO: 3448447341: === Top 10 Important Features ===]\n",
      "[2025-08-21 09:19:32,466: INFO: 3448447341: day_of_year: 0.2790]\n",
      "[2025-08-21 09:19:32,469: INFO: 3448447341: o3: 0.1751]\n",
      "[2025-08-21 09:19:32,478: INFO: 3448447341: o3_wind_interaction: 0.0570]\n",
      "[2025-08-21 09:19:32,487: INFO: 3448447341: wind_speed_mps: 0.0460]\n",
      "[2025-08-21 09:19:32,492: INFO: 3448447341: week_of_year: 0.0358]\n",
      "[2025-08-21 09:19:32,500: INFO: 3448447341: no2_co_ratio: 0.0335]\n",
      "[2025-08-21 09:19:32,503: INFO: 3448447341: aqi_rolling3_std: 0.0205]\n",
      "[2025-08-21 09:19:32,507: INFO: 3448447341: so2_wind_interaction: 0.0189]\n",
      "[2025-08-21 09:19:32,512: INFO: 3448447341: co: 0.0187]\n",
      "[2025-08-21 09:19:32,516: INFO: 3448447341: pm10_wind_interaction: 0.0186]\n",
      "[2025-08-21 09:19:32,518: INFO: 3448447341: Model training completed successfully!]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)  \n",
    "    model_trainer.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
