{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e7e83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b91bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Air-Quality-Health-Alert-System\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfadd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47144b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Air-Quality-Health-Alert-System'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da7ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    report_path: Path  \n",
    "    target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f28b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Air_Quality_Health_Alert_System.constants import *\n",
    "from Air_Quality_Health_Alert_System.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4eaf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_path=config.model_path,\n",
    "            report_path=config.report_path,\n",
    "            target_column=schema.name\n",
    "        )\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fb2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import json\n",
    "from Air_Quality_Health_Alert_System import logger\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b26bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.predictions = None   \n",
    "        self.actuals = None \n",
    "\n",
    "    def load_model_and_artifacts(self):\n",
    "        \n",
    "        if not os.path.exists(self.config.model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {self.config.model_path}\")\n",
    "        \n",
    "        model_artifacts = joblib.load(self.config.model_path)\n",
    "        \n",
    "        logger.info(\"Loaded model artifacts:\")\n",
    "        logger.info(f\"  Model type: {model_artifacts.get('model_type', 'Unknown')}\")\n",
    "        logger.info(f\"  Target column: {model_artifacts.get('target_column', 'Unknown')}\")\n",
    "        logger.info(f\"  Timestamp: {model_artifacts.get('timestamp', 'Unknown')}\")\n",
    "        logger.info(f\"  Features: {len(model_artifacts.get('feature_columns', []))}\")\n",
    "        \n",
    "        return model_artifacts\n",
    "\n",
    "    def load_test_data(self):\n",
    "      \n",
    "        if not os.path.exists(self.config.test_data_path):\n",
    "            raise FileNotFoundError(f\"Test data not found: {self.config.test_data_path}\")\n",
    "        \n",
    "    \n",
    "        if str(self.config.test_data_path).endswith('.csv'):\n",
    "            test_data = pd.read_csv(self.config.test_data_path)\n",
    "            logger.info(f\"Loaded test data from CSV: {test_data.shape}\")\n",
    "        elif str(self.config.test_data_path).endswith('.joblib'):\n",
    "            test_data = joblib.load(self.config.test_data_path)\n",
    "            logger.info(f\"Loaded test data from joblib: {type(test_data)}\")\n",
    "            \n",
    "            \n",
    "            if isinstance(test_data, dict):\n",
    "                if 'X_test' in test_data and 'y_test' in test_data:\n",
    "                    return test_data['X_test'], test_data['y_test']\n",
    "                else:\n",
    "                    raise ValueError(\"Joblib file doesn't contain 'X_test' and 'y_test' keys\")\n",
    "            elif isinstance(test_data, pd.DataFrame):\n",
    "                # If it's a DataFrame, we need to split features and target\n",
    "                if self.config.target_column in test_data.columns:\n",
    "                    X_test = test_data.drop(columns=[self.config.target_column])\n",
    "                    y_test = test_data[self.config.target_column]\n",
    "                    return X_test, y_test\n",
    "                else:\n",
    "                    raise ValueError(f\"Target column '{self.config.target_column}' not found in test data\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {self.config.test_data_path}\")\n",
    "        \n",
    "        # For CSV data, split features and target\n",
    "        if self.config.target_column in test_data.columns:\n",
    "            X_test = test_data.drop(columns=[self.config.target_column])\n",
    "            y_test = test_data[self.config.target_column]\n",
    "            logger.info(f\"Split test data - X: {X_test.shape}, y: {y_test.shape}\")\n",
    "            return X_test, y_test\n",
    "        else:\n",
    "            raise ValueError(f\"Target column '{self.config.target_column}' not found in test data\")\n",
    "\n",
    "    def validate_feature_compatibility(self, X_test, expected_features):\n",
    "        \n",
    "        current_features = set(X_test.columns)\n",
    "        expected_features_set = set(expected_features)\n",
    "        \n",
    "        missing_features = expected_features_set - current_features\n",
    "        extra_features = current_features - expected_features_set\n",
    "        \n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing features: {missing_features}\")\n",
    "            \n",
    "        if extra_features:\n",
    "            logger.warning(f\"Extra features: {extra_features}\")\n",
    "        \n",
    "        if missing_features or extra_features:\n",
    "            logger.info(\"Reordering test features to match model expectations...\")\n",
    "            X_test = X_test.reindex(columns=expected_features, fill_value=0)\n",
    "        \n",
    "        logger.info(f\"Feature compatibility check completed. Final shape: {X_test.shape}\")\n",
    "        return X_test\n",
    "\n",
    "    def calculate_comprehensive_metrics(self, y_true, y_pred):\n",
    "        \n",
    "        \n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "       \n",
    "        mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, 1e-8, y_true))) * 100\n",
    "        \n",
    "        \n",
    "        residuals = y_true - y_pred\n",
    "        mean_residual = np.mean(residuals)\n",
    "        std_residual = np.std(residuals)\n",
    "        \n",
    "       \n",
    "        pred_range = y_pred.max() - y_pred.min()\n",
    "        actual_range = y_true.max() - y_true.min()\n",
    "        range_coverage = (pred_range / actual_range * 100) if actual_range > 0 else 0\n",
    "        \n",
    "        \n",
    "        pred_stats = {\n",
    "            'mean': float(np.mean(y_pred)),\n",
    "            'std': float(np.std(y_pred)),\n",
    "            'min': float(np.min(y_pred)),\n",
    "            'max': float(np.max(y_pred))\n",
    "        }\n",
    "        \n",
    "        actual_stats = {\n",
    "            'mean': float(np.mean(y_true)),\n",
    "            'std': float(np.std(y_true)),\n",
    "            'min': float(np.min(y_true)),\n",
    "            'max': float(np.max(y_true))\n",
    "        }\n",
    "        \n",
    "        metrics = {\n",
    "            'mae': float(mae),\n",
    "            'rmse': float(rmse),\n",
    "            'r2': float(r2),\n",
    "            'mape': float(mape),\n",
    "            'mean_residual': float(mean_residual),\n",
    "            'std_residual': float(std_residual),\n",
    "            'range_coverage': float(range_coverage),\n",
    "            'n_samples': int(len(y_true)),\n",
    "            'prediction_stats': pred_stats,\n",
    "            'actual_stats': actual_stats\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def interpret_results(self, metrics):\n",
    "     \n",
    "        logger.info(\"=== MODEL EVALUATION RESULTS ===\")\n",
    "        \n",
    "       \n",
    "        logger.info(f\"MAE (Mean Absolute Error): {metrics['mae']:.4f}\")\n",
    "        logger.info(f\"RMSE (Root Mean Square Error): {metrics['rmse']:.4f}\")\n",
    "        logger.info(f\"R² (R-squared): {metrics['r2']:.4f}\")\n",
    "        logger.info(f\"MAPE (Mean Absolute Percentage Error): {metrics['mape']:.2f}%\")\n",
    "        logger.info(f\"Number of test samples: {metrics['n_samples']}\")\n",
    "        \n",
    "        \n",
    "        r2 = metrics['r2']\n",
    "        mape = metrics['mape']\n",
    "        \n",
    "        logger.info(\"\\n=== PERFORMANCE INTERPRETATION ===\")\n",
    "        \n",
    "        # R² interpretation\n",
    "        if r2 < 0.3:\n",
    "            logger.info(\"LOW R²: Model explains <30% of variance\")\n",
    "            logger.info(\"   Recommendation: Consider more features, different algorithm, or data quality review\")\n",
    "        elif r2 < 0.7:\n",
    "            logger.info(\"MODERATE R²: Room for improvement\")\n",
    "            logger.info(\"   Recommendation: Feature engineering or hyperparameter tuning\")\n",
    "        else:\n",
    "            logger.info(\"GOOD R²: Model performs well\")\n",
    "            \n",
    "        # MAPE interpretation\n",
    "        if mape > 20:\n",
    "            logger.info(\"HIGH MAPE: >20% prediction error\")\n",
    "            logger.info(\"   Recommendation: Model needs significant improvement\")\n",
    "        elif mape > 10:\n",
    "            logger.info(\"MODERATE MAPE: 10-20% prediction error\")\n",
    "            logger.info(\"   Recommendation: Acceptable for some use cases, consider refinement\")\n",
    "        else:\n",
    "            logger.info(\"LOW MAPE: <10% prediction error\")\n",
    "            logger.info(\"   Assessment: Good prediction accuracy\")\n",
    "        \n",
    "        \n",
    "        range_coverage = metrics['range_coverage']\n",
    "        logger.info(f\"\\nRange Coverage: {range_coverage:.1f}%\")\n",
    "        if range_coverage < 50:\n",
    "            logger.info(\"Low range coverage (<50%) - Model may be underfitting\")\n",
    "        elif range_coverage > 120:\n",
    "            logger.info(\"High range coverage (>120%) - Model may be overpredicting variance\")\n",
    "        else:\n",
    "            logger.info(\"Good range coverage\")\n",
    "        \n",
    "        \n",
    "        mean_residual = metrics['mean_residual']\n",
    "        if abs(mean_residual) > metrics['mae'] * 0.1:\n",
    "            logger.info(f\"Systematic bias detected (mean residual: {mean_residual:.4f})\")\n",
    "        else:\n",
    "            logger.info(\"No significant systematic bias\")\n",
    "        \n",
    "       \n",
    "        logger.info(f\"\\n=== PREDICTION STATISTICS ===\")\n",
    "        logger.info(f\"Prediction Range: [{metrics['prediction_stats']['min']:.2f}, {metrics['prediction_stats']['max']:.2f}]\")\n",
    "        logger.info(f\"Actual Range: [{metrics['actual_stats']['min']:.2f}, {metrics['actual_stats']['max']:.2f}]\")\n",
    "        logger.info(f\"Prediction Mean: {metrics['prediction_stats']['mean']:.2f}\")\n",
    "        logger.info(f\"Actual Mean: {metrics['actual_stats']['mean']:.2f}\")\n",
    "\n",
    "    def save_detailed_results(self, metrics, model_info):\n",
    "      \n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        \n",
    "        \n",
    "        complete_results = {\n",
    "            'evaluation_timestamp': datetime.now().isoformat(),\n",
    "            'model_info': {\n",
    "                'model_type': model_info.get('model_type', 'Unknown'),\n",
    "                'target_column': model_info.get('target_column', 'Unknown'),\n",
    "                'model_timestamp': model_info.get('timestamp', 'Unknown'),\n",
    "                'feature_count': len(model_info.get('feature_columns', []))\n",
    "            },\n",
    "            'metrics': metrics,\n",
    "            'evaluation_config': {\n",
    "                'model_path': str(self.config.model_path),\n",
    "                'test_data_path': str(self.config.test_data_path)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "      \n",
    "        results_path = os.path.join(self.config.root_dir, \"evaluation_results.json\")\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(complete_results, f, indent=4)\n",
    "        \n",
    "        logger.info(f\"Detailed results saved to: {results_path}\")\n",
    "        \n",
    "     \n",
    "        if self.predictions is not None and self.actuals is not None:\n",
    "            predictions_df = pd.DataFrame({\n",
    "                'actual': self.actuals.values if hasattr(self.actuals, 'values') else self.actuals,\n",
    "                'predicted': self.predictions,\n",
    "                'residual': (self.actuals.values if hasattr(self.actuals, 'values') else self.actuals) - self.predictions,\n",
    "                'abs_residual': np.abs((self.actuals.values if hasattr(self.actuals, 'values') else self.actuals) - self.predictions),\n",
    "                'percentage_error': np.abs(((self.actuals.values if hasattr(self.actuals, 'values') else self.actuals) - self.predictions) / \n",
    "                                         np.where((self.actuals.values if hasattr(self.actuals, 'values') else self.actuals) == 0, 1e-8, \n",
    "                                                (self.actuals.values if hasattr(self.actuals, 'values') else self.actuals))) * 100\n",
    "            })\n",
    "            \n",
    "            predictions_path = os.path.join(self.config.root_dir, \"predictions_vs_actuals.csv\")\n",
    "            predictions_df.to_csv(predictions_path, index=False)\n",
    "            logger.info(f\"Predictions vs actuals saved to: {predictions_path}\")\n",
    "        \n",
    "       \n",
    "        legacy_results = {\n",
    "            \"MAE\": metrics['mae'],\n",
    "            \"RMSE\": metrics['rmse'],\n",
    "            \"R2\": metrics['r2'],\n",
    "            \"MAPE\": metrics['mape'],\n",
    "            \"n_samples\": metrics['n_samples'],\n",
    "            \"target_column\": model_info.get('target_column', 'Unknown'),\n",
    "            \"model_type\": model_info.get('model_type', 'Unknown')\n",
    "        }\n",
    "        \n",
    "       \n",
    "        if hasattr(self.config, 'report_path') and self.config.report_path:\n",
    "            os.makedirs(os.path.dirname(self.config.report_path), exist_ok=True)\n",
    "            with open(self.config.report_path, \"w\") as f:\n",
    "                json.dump(legacy_results, f, indent=4)\n",
    "            logger.info(f\"Legacy report saved to: {self.config.report_path}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \n",
    "        logger.info(\"Starting model evaluation...\")\n",
    "        \n",
    "        try:\n",
    "           \n",
    "            model_artifacts = self.load_model_and_artifacts()\n",
    "            model = model_artifacts['model']\n",
    "            \n",
    "            \n",
    "            X_test, y_test = self.load_test_data()\n",
    "            \n",
    "            \n",
    "            expected_features = model_artifacts.get('feature_columns', [])\n",
    "            if expected_features:\n",
    "                X_test = self.validate_feature_compatibility(X_test, expected_features)\n",
    "            \n",
    "            \n",
    "            logger.info(\"Making predictions...\")\n",
    "            y_pred = model.predict(X_test)\n",
    "            logger.info(f\"Predictions generated for {len(y_pred)} samples\")\n",
    "\n",
    "           \n",
    "            self.predictions = y_pred\n",
    "            self.actuals = y_test\n",
    "            \n",
    "            \n",
    "            metrics = self.calculate_comprehensive_metrics(y_test, y_pred)\n",
    "            \n",
    "        \n",
    "            self.interpret_results(metrics)\n",
    "            \n",
    "            \n",
    "            self.save_detailed_results(metrics, model_artifacts)\n",
    "            \n",
    "            logger.info(\"Model evaluation completed successfully!\")\n",
    "            \n",
    "            return {\n",
    "                'metrics': metrics,\n",
    "                'model_info': model_artifacts,\n",
    "                'evaluation_completed': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model evaluation: {str(e)}\")\n",
    "            logger.error(f\"Error type: {type(e).__name__}\")\n",
    "            raise e\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719c867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-23 14:48:19,752: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-08-23 14:48:19,761: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-23 14:48:19,767: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-08-23 14:48:19,770: INFO: common: created directory at: artifacts]\n",
      "[2025-08-23 14:48:19,775: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2025-08-23 14:48:19,778: INFO: 447502381: Starting model evaluation...]\n",
      "[2025-08-23 14:48:19,870: INFO: 447502381: Loaded model artifacts:]\n",
      "[2025-08-23 14:48:19,871: INFO: 447502381:   Model type: XGBRegressor]\n",
      "[2025-08-23 14:48:19,873: INFO: 447502381:   Target column: aqi]\n",
      "[2025-08-23 14:48:19,874: INFO: 447502381:   Timestamp: 2025-08-23T14:41:47.591032]\n",
      "[2025-08-23 14:48:19,876: INFO: 447502381:   Features: 43]\n",
      "[2025-08-23 14:48:20,239: INFO: 447502381: Loaded test data from CSV: (30651, 45)]\n",
      "[2025-08-23 14:48:20,246: INFO: 447502381: Split test data - X: (30651, 44), y: (30651,)]\n",
      "[2025-08-23 14:48:20,247: WARNING: 447502381: Missing features: {'season_Summer', 'season_Winter', 'season_Spring'}]\n",
      "[2025-08-23 14:48:20,248: WARNING: 447502381: Extra features: {'date', 'AQI_Category', 'season', 'city'}]\n",
      "[2025-08-23 14:48:20,249: INFO: 447502381: Reordering test features to match model expectations...]\n",
      "[2025-08-23 14:48:20,255: INFO: 447502381: Feature compatibility check completed. Final shape: (30651, 43)]\n",
      "[2025-08-23 14:48:20,257: INFO: 447502381: Making predictions...]\n",
      "[2025-08-23 14:48:20,275: INFO: 447502381: Predictions generated for 30651 samples]\n",
      "[2025-08-23 14:48:20,283: INFO: 447502381: === MODEL EVALUATION RESULTS ===]\n",
      "[2025-08-23 14:48:20,285: INFO: 447502381: MAE (Mean Absolute Error): 0.0289]\n",
      "[2025-08-23 14:48:20,286: INFO: 447502381: RMSE (Root Mean Square Error): 0.0731]\n",
      "[2025-08-23 14:48:20,288: INFO: 447502381: R² (R-squared): 0.2655]\n",
      "[2025-08-23 14:48:20,290: INFO: 447502381: MAPE (Mean Absolute Percentage Error): 19.91%]\n",
      "[2025-08-23 14:48:20,292: INFO: 447502381: Number of test samples: 30651]\n",
      "[2025-08-23 14:48:20,293: INFO: 447502381: \n",
      "=== PERFORMANCE INTERPRETATION ===]\n",
      "[2025-08-23 14:48:20,294: INFO: 447502381: LOW R²: Model explains <30% of variance]\n",
      "[2025-08-23 14:48:20,295: INFO: 447502381:    Recommendation: Consider more features, different algorithm, or data quality review]\n",
      "[2025-08-23 14:48:20,297: INFO: 447502381: MODERATE MAPE: 10-20% prediction error]\n",
      "[2025-08-23 14:48:20,298: INFO: 447502381:    Recommendation: Acceptable for some use cases, consider refinement]\n",
      "[2025-08-23 14:48:20,300: INFO: 447502381: \n",
      "Range Coverage: 27.8%]\n",
      "[2025-08-23 14:48:20,302: INFO: 447502381: Low range coverage (<50%) - Model may be underfitting]\n",
      "[2025-08-23 14:48:20,304: INFO: 447502381: No significant systematic bias]\n",
      "[2025-08-23 14:48:20,306: INFO: 447502381: \n",
      "=== PREDICTION STATISTICS ===]\n",
      "[2025-08-23 14:48:20,308: INFO: 447502381: Prediction Range: [0.07, 0.35]]\n",
      "[2025-08-23 14:48:20,310: INFO: 447502381: Actual Range: [0.00, 1.00]]\n",
      "[2025-08-23 14:48:20,311: INFO: 447502381: Prediction Mean: 0.15]\n",
      "[2025-08-23 14:48:20,312: INFO: 447502381: Actual Mean: 0.15]\n",
      "[2025-08-23 14:48:20,315: INFO: 447502381: Detailed results saved to: artifacts/model_evaluation\\evaluation_results.json]\n",
      "[2025-08-23 14:48:20,576: INFO: 447502381: Predictions vs actuals saved to: artifacts/model_evaluation\\predictions_vs_actuals.csv]\n",
      "[2025-08-23 14:48:20,578: INFO: 447502381: Legacy report saved to: artifacts/model_evaluation/metrics.json]\n",
      "[2025-08-23 14:48:20,579: INFO: 447502381: Model evaluation completed successfully!]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluator = ModelEvaluation(config=model_evaluation_config)  \n",
    "    model_evaluator.evaluate()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7af006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e0a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
