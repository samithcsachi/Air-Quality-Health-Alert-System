{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e7e83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b91bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Air-Quality-Health-Alert-System\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfadd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47144b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Air-Quality-Health-Alert-System'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da7ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    report_path: Path  \n",
    "    target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f28b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Air_Quality_Health_Alert_System.constants import *\n",
    "from Air_Quality_Health_Alert_System.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4eaf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_path=config.model_path,\n",
    "            report_path=config.report_path,\n",
    "            target_column=schema.name\n",
    "        )\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fb2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def create_lag_features(self, df, target_col='aqi', lags=[1, 2, 3], rolling_windows=[3, 7]):\n",
    "       \n",
    "        df = df.copy()\n",
    "        \n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Creating lag features for {target_col}...\")\n",
    "        \n",
    "       \n",
    "        for lag in lags:\n",
    "            df[f\"{target_col}_lag{lag}\"] = df[target_col].shift(lag)\n",
    "            \n",
    "       \n",
    "        for window in rolling_windows:\n",
    "            df[f\"{target_col}_rolling{window}\"] = df[target_col].shift(1).rolling(window=window).mean()\n",
    "            df[f\"{target_col}_rolling{window}_std\"] = df[target_col].shift(1).rolling(window=window).std()\n",
    "        \n",
    " \n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna()\n",
    "        final_rows = len(df)\n",
    "        \n",
    "        print(f\"Lag features created. Rows: {initial_rows} -> {final_rows} (removed {initial_rows - final_rows} NaN rows)\")\n",
    "        return df\n",
    "\n",
    "    def create_seasonal_features(self, df):\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'season' in df.columns:\n",
    "            print(\"Creating one-hot encoded seasonal features...\")\n",
    "           \n",
    "            seasonal_dummies = pd.get_dummies(df['season'], prefix='season', drop_first=False)\n",
    "            \n",
    "           \n",
    "            df = pd.concat([df, seasonal_dummies], axis=1)\n",
    "            \n",
    "            \n",
    "            df = df.drop('season', axis=1)\n",
    "            \n",
    "            print(f\"Seasonal features created: {[col for col in seasonal_dummies.columns]}\")\n",
    "        else:\n",
    "            print(\"Warning: 'season' column not found!\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def prepare_test_features(self, test_data, expected_features, target_column):\n",
    "        \n",
    "        \n",
    "        print(f\"Preparing test features...\")\n",
    "        print(f\"Initial test data shape: {test_data.shape}\")\n",
    "        \n",
    "        \n",
    "        test_data_with_lags = self.create_lag_features(test_data, target_column)\n",
    "        print(f\"After lag features: {test_data_with_lags.shape}\")\n",
    "        \n",
    "       \n",
    "        test_data_engineered = self.create_seasonal_features(test_data_with_lags)\n",
    "        print(f\"After seasonal features: {test_data_engineered.shape}\")\n",
    "        \n",
    "        \n",
    "        columns_to_remove = ['date', 'city', 'AQI_Category']\n",
    "        \n",
    "        available_cols = [col for col in test_data_engineered.columns \n",
    "                         if col not in columns_to_remove and col != target_column]\n",
    "        \n",
    "        X_test_temp = test_data_engineered[available_cols].copy()\n",
    "        y_test = test_data_engineered[target_column].copy()\n",
    "        \n",
    "        print(f\"Available feature columns: {len(available_cols)}\")\n",
    "        \n",
    "        \n",
    "        missing_features = []\n",
    "        for feature in expected_features:\n",
    "            if feature not in X_test_temp.columns:\n",
    "                missing_features.append(feature)\n",
    "              \n",
    "                if 'lag' in str(feature) or 'rolling' in str(feature):\n",
    "                  \n",
    "                    X_test_temp[feature] = y_test.mean()\n",
    "                else:\n",
    "                    \n",
    "                    X_test_temp[feature] = 0\n",
    "        \n",
    "        if missing_features:\n",
    "            print(f\"Added missing features with defaults: {missing_features}\")\n",
    "        \n",
    "        \n",
    "        expected_features_clean = [str(f) for f in expected_features]\n",
    "        \n",
    "      \n",
    "        for feature in expected_features_clean:\n",
    "            if feature not in X_test_temp.columns:\n",
    "                print(f\"Warning: Expected feature '{feature}' still missing, adding with default value\")\n",
    "                X_test_temp[feature] = 0\n",
    "        \n",
    "        X_test = X_test_temp[expected_features_clean].copy()\n",
    "        \n",
    "        print(f\"Final feature preparation:\")\n",
    "        print(f\"  X_test shape: {X_test.shape}\")\n",
    "        print(f\"  y_test shape: {y_test.shape}\")\n",
    "        print(f\"  Features match expected: {len(X_test.columns) == len(expected_features)}\")\n",
    "        \n",
    "        return X_test, y_test\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"Starting model evaluation...\")\n",
    "        \n",
    "     \n",
    "        model_artifacts = joblib.load(self.config.model_path)\n",
    "        model = model_artifacts['model']\n",
    "        target_column = model_artifacts['target_column']\n",
    "        \n",
    "        print(f\"Loaded model artifacts:\")\n",
    "        print(f\"  Model type: {model_artifacts.get('model_type', 'Unknown')}\")\n",
    "        print(f\"  Target column: {target_column}\")\n",
    "        \n",
    "        \n",
    "        if hasattr(model, 'feature_names_in_') and model.feature_names_in_ is not None:\n",
    "            expected_features = model.feature_names_in_\n",
    "            print(f\"Using model's feature_names_in_: {len(expected_features)} features\")\n",
    "        else:\n",
    "            expected_features = model_artifacts['feature_columns']\n",
    "            print(f\"Using saved feature_columns: {len(expected_features)} features\")\n",
    "        \n",
    "      \n",
    "        test_data = pd.read_csv(self.config.test_data_path, parse_dates=['date'])\n",
    "        print(f\"Test data loaded: {test_data.shape}\")\n",
    "        \n",
    "       \n",
    "        X_test, y_test = self.prepare_test_features(test_data, expected_features, target_column)\n",
    "        \n",
    "        \n",
    "        scaler = model_artifacts.get('scaler')\n",
    "        if scaler is not None:\n",
    "            print(\"Checking scaler compatibility...\")\n",
    "            \n",
    "           \n",
    "            if hasattr(scaler, 'feature_names_in_'):\n",
    "                scaler_features = scaler.feature_names_in_\n",
    "                print(f\"Scaler was fitted on {len(scaler_features)} features\")\n",
    "                \n",
    "                \n",
    "                current_features = set(X_test.columns)\n",
    "                scaler_features_set = set(scaler_features)\n",
    "                \n",
    "                if current_features == scaler_features_set:\n",
    "                    print(\"Scaler features match perfectly, applying scaling...\")\n",
    "                    X_test_scaled = pd.DataFrame(\n",
    "                        scaler.transform(X_test),\n",
    "                        columns=X_test.columns,\n",
    "                        index=X_test.index\n",
    "                    )\n",
    "                    X_test = X_test_scaled\n",
    "                    print(\"Scaling applied successfully\")\n",
    "                else:\n",
    "                    print(\"Scaler feature mismatch detected:\")\n",
    "                    print(f\"  Missing in current: {scaler_features_set - current_features}\")\n",
    "                    print(f\"  Extra in current: {current_features - scaler_features_set}\")\n",
    "                    print(\"Skipping scaling to avoid errors...\")\n",
    "            else:\n",
    "                print(\"Scaler doesn't have feature_names_in_, attempting to apply scaling...\")\n",
    "                try:\n",
    "                    X_test_scaled = pd.DataFrame(\n",
    "                        scaler.transform(X_test),\n",
    "                        columns=X_test.columns,\n",
    "                        index=X_test.index\n",
    "                    )\n",
    "                    X_test = X_test_scaled\n",
    "                    print(\"Scaling applied successfully\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Scaling failed: {e}\")\n",
    "                    print(\"Proceeding without scaling...\")\n",
    "        else:\n",
    "            print(\"No scaler found in model artifacts\")\n",
    "        \n",
    "     \n",
    "        print(\"Making predictions...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"Predictions generated for {len(y_pred)} samples\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1, y_test))) * 100\n",
    "        \n",
    "        results = {\n",
    "            \"MAE\": float(mae),\n",
    "            \"RMSE\": float(rmse),\n",
    "            \"R2\": float(r2),\n",
    "            \"MAPE\": float(mape),\n",
    "            \"n_samples\": int(len(y_test)),\n",
    "            \"target_column\": target_column,\n",
    "            \"model_type\": model_artifacts.get('model_type', 'Unknown')\n",
    "        }\n",
    "        \n",
    "       \n",
    "        os.makedirs(os.path.dirname(self.config.report_path), exist_ok=True)\n",
    "        with open(self.config.report_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "       \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "        print(f\"RMSE (Root Mean Square Error): {rmse:.4f}\")\n",
    "        print(f\"R² (R-squared): {r2:.4f}\")\n",
    "        print(f\"MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "        print(f\"Number of test samples: {len(y_test)}\")\n",
    "        \n",
    "       \n",
    "        print(f\"\\n\" + \"=\"*30)\n",
    "        print(\"PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        if r2 < 0.3:\n",
    "            print(\"LOW R²: Model explains <30% of variance\")\n",
    "            print(\"   Consider: More features, different algorithm, or data quality issues\")\n",
    "        elif r2 < 0.7:\n",
    "            print(\"MODERATE R²: Room for improvement\")\n",
    "        else:\n",
    "            print(\"GOOD R²: Model performs well\")\n",
    "            \n",
    "        if mape > 20:\n",
    "            print(\"HIGH MAPE: >20% prediction error\")\n",
    "        elif mape > 10:\n",
    "            print(\" MODERATE MAPE: 10-20% prediction error\") \n",
    "        else:\n",
    "            print(\"LOW MAPE: <10% prediction error\")\n",
    "        \n",
    "        \n",
    "        pred_range = y_pred.max() - y_pred.min()\n",
    "        actual_range = y_test.max() - y_test.min()\n",
    "        range_coverage = pred_range / actual_range * 100\n",
    "        \n",
    "        print(f\"\\nRange Coverage: {range_coverage:.1f}%\")\n",
    "        if range_coverage < 50:\n",
    "            print(\"Model predictions cover <50% of actual value range\")\n",
    "            print(\"   This suggests the model may be underfitting\")\n",
    "        \n",
    "        print(f\"\\nResults saved to: {self.config.report_path}\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\nDETAILED STATISTICS:\")\n",
    "        print(f\"Prediction Range: [{y_pred.min():.2f}, {y_pred.max():.2f}]\")\n",
    "        print(f\"Actual Range: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n",
    "        print(f\"Mean Absolute Residual: {np.mean(np.abs(y_test - y_pred)):.4f}\")\n",
    "        print(f\"Prediction Std: {y_pred.std():.4f}\")\n",
    "        print(f\"Actual Std: {y_test.std():.4f}\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719c867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-21 22:27:37,185: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-08-21 22:27:37,191: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-21 22:27:37,197: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-08-21 22:27:37,199: INFO: common: created directory at: artifacts]\n",
      "[2025-08-21 22:27:37,201: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "Starting model evaluation...\n",
      "Loaded model artifacts:\n",
      "  Model type: XGBRegressor\n",
      "  Target column: aqi\n",
      "Using model's feature_names_in_: 50 features\n",
      "Test data loaded: (23282, 45)\n",
      "Preparing test features...\n",
      "Initial test data shape: (23282, 45)\n",
      "Creating lag features for aqi...\n",
      "Lag features created. Rows: 23282 -> 23275 (removed 7 NaN rows)\n",
      "After lag features: (23275, 52)\n",
      "Creating one-hot encoded seasonal features...\n",
      "Seasonal features created: ['season_Fall', 'season_Spring', 'season_Summer', 'season_Winter']\n",
      "After seasonal features: (23275, 55)\n",
      "Available feature columns: 51\n",
      "Final feature preparation:\n",
      "  X_test shape: (23275, 50)\n",
      "  y_test shape: (23275,)\n",
      "  Features match expected: True\n",
      "Checking scaler compatibility...\n",
      "Scaler was fitted on 26 features\n",
      "Scaler feature mismatch detected:\n",
      "  Missing in current: {'aqi'}\n",
      "  Extra in current: {'aqi_rolling3', 'month', 'week_of_year', 'is_summer', 'season_Summer', 'is_autumn', 'longitude', 'aqi_lag2', 'day_of_week', 'day', 'season_Winter', 'aqi_rolling7', 'is_weekend', 'day_of_year', 'is_spring', 'latitude', 'aqi_lag3', 'aqi_rolling7_std', 'season_Spring', 'quarter', 'year', 'is_winter', 'hour', 'aqi_rolling3_std', 'aqi_lag1'}\n",
      "Skipping scaling to avoid errors...\n",
      "Making predictions...\n",
      "Predictions generated for 23275 samples\n",
      "\n",
      "==================================================\n",
      "MODEL EVALUATION RESULTS\n",
      "==================================================\n",
      "MAE (Mean Absolute Error): 0.0309\n",
      "RMSE (Root Mean Square Error): 0.0792\n",
      "R² (R-squared): 0.2466\n",
      "MAPE (Mean Absolute Percentage Error): 24.22%\n",
      "Number of test samples: 23275\n",
      "\n",
      "==============================\n",
      "PERFORMANCE ANALYSIS\n",
      "==============================\n",
      "LOW R²: Model explains <30% of variance\n",
      "   Consider: More features, different algorithm, or data quality issues\n",
      "HIGH MAPE: >20% prediction error\n",
      "\n",
      "Range Coverage: 25.5%\n",
      "Model predictions cover <50% of actual value range\n",
      "   This suggests the model may be underfitting\n",
      "\n",
      "Results saved to: artifacts/model_evaluation/metrics.json\n",
      "\n",
      "DETAILED STATISTICS:\n",
      "Prediction Range: [0.07, 0.32]\n",
      "Actual Range: [0.00, 0.99]\n",
      "Mean Absolute Residual: 0.0309\n",
      "Prediction Std: 0.0441\n",
      "Actual Std: 0.0912\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluator = ModelEvaluation(config=model_evaluation_config)  \n",
    "    model_evaluator.evaluate()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e0a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
